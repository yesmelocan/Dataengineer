from airflow import DAG 
from airflow.contrib.operators.dataproc_operator import DataprocCreateClusterOperator,DataProcPySparkOperator,DataprocDeleteClusterOperator
import pendulum

PROJECT_ID="scenic-parity-429506-b8"

CLUSTER_NAME = "dataproc"

REGION = "us-west1"

CLUSTER_CONFIG = {
    "master_config": {
        "num_instances": 1,
        "machine_type_uri": "t2d-standard-1",
        "disk_config": {
            "boot_disk_type": "pd-balanced",
            "boot_disk_size_gb": 50
        }
    },
    "worker_config": {
        "num_instances": 2,
        "machine_type_uri": "t2d-standard-1",
        "disk_config": {
            "boot_disk_type": "pd-balanced",
            "boot_disk_size_gb": 50
        }
    }

}


dag = DAG(
    dag_id="DataProc_BigQuery",
    start_date=pendulum.datetime(2024, 8, 16),
    schedule_interval="@daily",
    catchup=False
)

create_cluster = DataprocCreateClusterOperator(
    task_id="create_cluster",
    project_id=PROJECT_ID,
    cluster_config=CLUSTER_CONFIG,
    region=REGION,
    cluster_name=CLUSTER_NAME,
    gcp_conn_id="google_cloud_default",
    dag=dag
)

pyspark_task = DataProcPySparkOperator(
    task_id="pyspark_task",
    main="gs://deneme12121/uber_data_transform.py",
    cluster_name=CLUSTER_NAME,
    region=REGION,
    dataproc_jars = ['gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar'],
    gcp_conn_id="google_cloud_default",
    dag=dag
)

delete_cluster = DataprocDeleteClusterOperator(
    task_id="delete_cluster",
    project_id=PROJECT_ID,
    cluster_name=CLUSTER_NAME,
    region=REGION,
    gcp_conn_id="google_cloud_default",
    dag=dag
)

create_cluster >> pyspark_task >> delete_cluster